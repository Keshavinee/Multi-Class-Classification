{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIqUJ9ZbfGlb",
        "outputId": "c13974b7-1ce7-4aa0-b6aa-7fda7add0197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rOc8hXUmtc7"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/drive/MyDrive/dataset.zip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import reqired libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "fifuFViC4qkO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LrwlblHme114"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load(lst):\n",
        "  data = []\n",
        "  for npy_file in lst:\n",
        "      npy_data = np.load(npy_file)\n",
        "      data.append(npy_data)\n",
        "  return data\n",
        "\n",
        "# Load the data\n",
        "no_images_list = glob.glob('/content/dataset/train/no/*.npy')\n",
        "sphere_images_list = glob.glob('/content/dataset/train/sphere/*.npy')\n",
        "vort_images_list = glob.glob('/content/dataset/train/vort/*.npy')\n",
        "\n",
        "no_images = load(no_images_list)\n",
        "sphere_images = load(sphere_images_list)\n",
        "vort_images = load(vort_images_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the data into a single array\n",
        "X = np.concatenate((no_images[:3000], sphere_images[:3000], vort_images[:3000]), axis=0)\n",
        "\n",
        "# Create labels for the data\n",
        "y = np.concatenate((np.zeros(3000), np.ones(3000), np.ones(3000)*2), axis=0)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Define hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "KaJxjVaj6Vgk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # validation data\n",
        "# no_images_v_list = glob.glob('/content/dataset/val/no/*.npy')\n",
        "# sphere_images_v_list = glob.glob('/content/dataset/val/sphere/*.npy')\n",
        "# vort_images_v_list = glob.glob('/content/dataset/val/vort/*.npy')\n",
        "\n",
        "# no_images_v = load(no_images_v_list)\n",
        "# sphere_images_v = load(sphere_images_v_list)\n",
        "# vort_images_v = load(vort_images_v_list)"
      ],
      "metadata": {
        "id": "eeWD0g0M5qhS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_val = np.concatenate((no_images_v[:500], sphere_images_v[:500], vort_images_v[:500]), axis=0)\n",
        "# y_val = np.concatenate((np.zeros(500), np.ones(500), np.ones(500)*2), axis=0)"
      ],
      "metadata": {
        "id": "R7lzTXeG64qH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3-GUBO3TgXHL"
      },
      "outputs": [],
      "source": [
        "# Create the training and testing datasets\n",
        "train_dataset = torch.utils.data.TensorDataset(\n",
        "    torch.from_numpy(X_train.astype(np.float32)),\n",
        "    torch.from_numpy(y_train.astype(np.int64))\n",
        ")\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(\n",
        "    torch.from_numpy(X_test.astype(np.float32)),\n",
        "    torch.from_numpy(y_test.astype(np.int64))\n",
        ")\n",
        "\n",
        "val_dataset = torch.utils.data.TensorDataset(\n",
        "    torch.from_numpy(X_test.astype(np.float32)),\n",
        "    torch.from_numpy(y_test.astype(np.int64))\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btXb3vzFgpmz"
      },
      "source": [
        "Model Architecture: The model architecture is a simple convolutional neural network (CNN) consisting of three convolutional layers, each followed by a ReLU activation and max-pooling operation to reduce the spatial dimensions of the output feature maps. The output of the last convolutional layer is flattened and passed through two fully connected layers, each followed by a ReLU activation function, and a final softmax layer for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9eg-A1sXgoId"
      },
      "outputs": [],
      "source": [
        "#Define model architecture\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(64 * 18 * 18, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 3),\n",
        "    nn.Softmax(dim=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tID7AJbzg3jl"
      },
      "source": [
        "Loss Function, Optimizer, and Evaluation Metric: The loss function used is Cross Entropy Loss, which is commonly used for multi-class classification problems. The optimizer used is the Adam optimizer, which is a widely used stochastic gradient descent optimizer with adaptive learning rates. The evaluation metric used is the Area Under the Receiver Operating Characteristic Curve (ROC AUC), which measures the model's ability to distinguish between the three classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yK8MloD5g4nw"
      },
      "outputs": [],
      "source": [
        "# Define loss function, optimizer, and evaluation metric\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDKKa-3XhLXm"
      },
      "source": [
        "Training the Model: The model is trained using the training dataset and validated using the validation dataset. The number of epochs used is 10, and the batch size is 32. The model is evaluated after each epoch using the validation set, and the loss and AUC score are printed for each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX3F4ZoTgVhj"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    running_loss /= len(train_loader.dataset)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss:.4f}')\n",
        "\n",
        "torch.save(model.state_dict(), 'lens_classification_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXQL0x9LhO7j"
      },
      "source": [
        "Evaluating the Model: Once the model is trained, it is evaluated on the test dataset, and the AUC score and ROC AUC score are computed. The AUC score is computed using the predicted and actual labels, and the ROC curve is plotted using the predicted scores and actual labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wnklulBhOGt"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on test data\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "predictions = []\n",
        "actuals = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item() * images.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "        test_total += labels.size(0)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(labels.cpu().numpy())\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_accuracy = test_correct / test_total\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Calculate ROC curve and AUC score\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import numpy as np\n",
        "predictions = np.array(predictions)\n",
        "actuals = np.array(actuals)\n",
        "fpr, tpr, thresholds = roc_curve(actuals, predictions[:, 1], pos_label=1)\n",
        "auc_score = roc_auc_score(actuals, predictions,multi_class='ovo')\n",
        "print(f'AUC Score: {auc_score:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROC Curve"
      ],
      "metadata": {
        "id": "zxjFz9qKmoME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot the ROC curve\n",
        "plt.plot(fpr, tpr, label='ROC curve (AUC = {:.2f})'.format(auc_score))\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "019aeX3wmqUQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}